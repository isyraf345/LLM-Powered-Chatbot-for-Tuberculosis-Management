{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7qRtru92vHrV",
        "outputId": "9c91960b-ad2d-42ed-c0f3-1dabfc97c147"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.2)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.12.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (5.1 kB)\n",
            "Collecting pypdf\n",
            "  Downloading pypdf-6.3.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: langchain in /usr/local/lib/python3.12/dist-packages (0.3.27)\n",
            "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.57.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (2.8.0+cu126)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.6.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.36.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.15.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (2.0.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from faiss-cpu) (25.0)\n",
            "Requirement already satisfied: langchain-core<1.0.0,>=0.3.72 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.79)\n",
            "Requirement already satisfied: langchain-text-splitters<1.0.0,>=0.3.9 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.3.11)\n",
            "Requirement already satisfied: langsmith>=0.1.17 in /usr/local/lib/python3.12/dist-packages (from langchain) (0.4.42)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.11.10)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.0.44)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.12/dist-packages (from langchain) (2.32.4)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.12/dist-packages (from langchain) (6.0.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (3.20.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.3.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (8.5.0)\n",
            "Requirement already satisfied: jsonpatch<2.0.0,>=1.33.0 in /usr/local/lib/python3.12/dist-packages (from langchain-core<1.0.0,>=0.3.72->langchain) (1.33)\n",
            "Requirement already satisfied: httpx<1,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.28.1)\n",
            "Requirement already satisfied: orjson>=3.9.14 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (3.11.4)\n",
            "Requirement already satisfied: requests-toolbelt>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (1.0.0)\n",
            "Requirement already satisfied: zstandard>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from langsmith>=0.1.17->langchain) (0.25.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.12/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.4.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2->langchain) (2025.10.5)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.12/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.2.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.11.0->sentence-transformers) (3.4.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2024.11.6)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (4.11.0)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.12/dist-packages (from httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.12/dist-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (0.16.0)\n",
            "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.12/dist-packages (from jsonpatch<2.0.0,>=1.33.0->langchain-core<1.0.0,>=0.3.72->langchain) (3.0.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.3)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.12/dist-packages (from anyio->httpx<1,>=0.23.0->langsmith>=0.1.17->langchain) (1.3.1)\n",
            "Downloading faiss_cpu-1.12.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (31.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.4/31.4 MB\u001b[0m \u001b[31m79.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pypdf-6.3.0-py3-none-any.whl (328 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m328.9/328.9 kB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pypdf, faiss-cpu\n",
            "Successfully installed faiss-cpu-1.12.0 pypdf-6.3.0\n"
          ]
        }
      ],
      "source": [
        "!pip install sentence-transformers faiss-cpu pypdf langchain"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YI4b7No6vq0y",
        "outputId": "3db42447-9599-44cf-c899-156ada9dbf91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ──────────────────────────────────────────────────────────────\n",
        "# CELL 3: Full Classes (DocumentProcessor + DPRRetriever + NLIVerifier)\n",
        "# ──────────────────────────────────────────────────────────────\n",
        "import os\n",
        "import json\n",
        "import numpy as np\n",
        "from typing import List, Dict\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from transformers import AutoTokenizer\n",
        "import faiss\n",
        "import torch\n",
        "from pypdf import PdfReader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from transformers import pipeline\n",
        "\n",
        "# ======================================================================\n",
        "# 1. DOCUMENT PROCESSOR (chunk = 400)\n",
        "# ======================================================================\n",
        "class DocumentProcessor:\n",
        "    def __init__(self, chunk_size=300, chunk_overlap=50):\n",
        "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=chunk_size,\n",
        "            chunk_overlap=chunk_overlap,\n",
        "            separators=[\"\\n\\n\", \"\\n\", \". \", \"! \", \"? \", \" \", \"\"]\n",
        "        )\n",
        "\n",
        "    def load_document(self, file_path: str) -> Dict:\n",
        "        _, ext = os.path.splitext(file_path)\n",
        "        if ext.lower() == \".pdf\":\n",
        "            reader = PdfReader(file_path)\n",
        "            text = \"\\n\".join([page.extract_text() or \"\" for page in reader.pages])\n",
        "        else:\n",
        "            with open(file_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "                text = f.read()\n",
        "        return {\"source\": os.path.basename(file_path), \"content\": text}\n",
        "\n",
        "    def process_documents(self, file_paths: List[str]) -> List[Dict]:\n",
        "        chunks = []\n",
        "        for path in file_paths:\n",
        "            print(f\"Loading: {path}\")\n",
        "            doc = self.load_document(path)\n",
        "            parts = self.text_splitter.split_text(doc[\"content\"])\n",
        "            for idx, text in enumerate(parts):\n",
        "                chunks.append({\n",
        "                    \"text\": text.strip(),\n",
        "                    \"source\": doc[\"source\"],\n",
        "                    \"chunk_id\": idx\n",
        "                })\n",
        "        return chunks\n",
        "\n",
        "# --------------------------------------------------------------\n",
        "#  DPR RETRIEVER – uses the official DPR context encoder\n",
        "# --------------------------------------------------------------\n",
        "from transformers import DPRContextEncoder, DPRContextEncoderTokenizerFast\n",
        "import torch, faiss, os, json, numpy as np\n",
        "from typing import List, Dict\n",
        "\n",
        "class DPRRetriever:\n",
        "    def __init__(self, model_name: str = \"facebook/dpr-ctx_encoder-multiset-base\"):\n",
        "        self.model_name = model_name\n",
        "        print(f\"Loading DPR context encoder: {model_name}\")\n",
        "        self.tokenizer = DPRContextEncoderTokenizerFast.from_pretrained(model_name)\n",
        "        self.model     = DPRContextEncoder.from_pretrained(model_name)\n",
        "        self.model.eval()\n",
        "        self.index = None\n",
        "        self.chunks = []\n",
        "        self.embeddings = None\n",
        "\n",
        "    # --------------------------------------------------------------\n",
        "    #  Build FAISS index from list of chunks\n",
        "    # --------------------------------------------------------------\n",
        "    def build_index(self, chunks: List[Dict]):\n",
        "        self.chunks = chunks\n",
        "        texts = [c[\"text\"] for c in chunks]\n",
        "\n",
        "        batch_size = 16                     # safe for Colab GPU/CPU\n",
        "        all_emb    = []\n",
        "\n",
        "        print(f\"Encoding {len(texts)} chunks (max 512 tokens)…\")\n",
        "        with torch.no_grad():\n",
        "            for i in range(0, len(texts), batch_size):\n",
        "                batch = texts[i:i+batch_size]\n",
        "                inputs = self.tokenizer(\n",
        "                    batch,\n",
        "                    padding=True,\n",
        "                    truncation=True,\n",
        "                    max_length=512,\n",
        "                    return_tensors=\"pt\"\n",
        "                )\n",
        "                # DPRContextEncoder returns (pooler_output, …)\n",
        "                outputs = self.model(**inputs)\n",
        "                # pooler_output is already L2-normalised by DPR\n",
        "                emb = outputs.pooler_output.cpu().numpy()\n",
        "                all_emb.append(emb)\n",
        "\n",
        "                print(f\"  → {min(i+batch_size, len(texts))}/{len(texts)}\", end=\"\\r\")\n",
        "\n",
        "        self.embeddings = np.vstack(all_emb).astype(\"float32\")\n",
        "        dim = self.embeddings.shape[1]\n",
        "        self.index = faiss.IndexFlatIP(dim)\n",
        "        self.index.add(self.embeddings)\n",
        "        print(f\"\\nFAISS index built – {self.index.ntotal} vectors\")\n",
        "\n",
        "    # --------------------------------------------------------------\n",
        "    #  Save index + metadata\n",
        "    # --------------------------------------------------------------\n",
        "    def save_index(self, path: str = \"index\"):\n",
        "        os.makedirs(path, exist_ok=True)\n",
        "        faiss.write_index(self.index, f\"{path}/faiss.index\")\n",
        "        with open(f\"{path}/chunks.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "            json.dump(self.chunks, f)\n",
        "        np.save(f\"{path}/embeddings.npy\", self.embeddings)\n",
        "        print(f\"Index saved to {path}/\")\n",
        "\n",
        "    # --------------------------------------------------------------\n",
        "    #  Load a previously saved index\n",
        "    # --------------------------------------------------------------\n",
        "    def load_index(self, path: str = \"index\"):\n",
        "        self.index = faiss.read_index(f\"{path}/faiss.index\")\n",
        "        with open(f\"{path}/chunks.json\", encoding=\"utf-8\") as f:\n",
        "            self.chunks = json.load(f)\n",
        "        self.embeddings = np.load(f\"{path}/embeddings.npy\")\n",
        "        # reload model/tokenizer (needed for retrieval)\n",
        "        self.tokenizer = DPRContextEncoderTokenizerFast.from_pretrained(self.model_name)\n",
        "        self.model     = DPRContextEncoder.from_pretrained(self.model_name)\n",
        "        self.model.eval()\n",
        "        print(f\"Index loaded from {path}/ ({self.index.ntotal} vectors)\")\n",
        "\n",
        "    # --------------------------------------------------------------\n",
        "    #  Retrieve top-k passages for a query\n",
        "    # --------------------------------------------------------------\n",
        "    def retrieve(self, query: str, top_k: int = 3) -> List[Dict]:\n",
        "        with torch.no_grad():\n",
        "            inputs = self.tokenizer(\n",
        "                [query],\n",
        "                padding=True,\n",
        "                truncation=True,\n",
        "                max_length=512,\n",
        "                return_tensors=\"pt\"\n",
        "            )\n",
        "            q_emb = self.model(**inputs).pooler_output.cpu().numpy().astype(\"float32\")\n",
        "\n",
        "        distances, indices = self.index.search(q_emb, top_k)\n",
        "        results = []\n",
        "        for dist, idx in zip(distances[0], indices[0]):\n",
        "            chunk = self.chunks[idx]\n",
        "            results.append({\n",
        "                \"text\":   chunk[\"text\"],\n",
        "                \"source\": chunk[\"source\"],\n",
        "                \"score\":  float(dist)\n",
        "            })\n",
        "        return results\n",
        "# ======================================================================\n",
        "# 3. NLI VERIFIER\n",
        "# ======================================================================\n",
        "class NLIVerifier:\n",
        "    def __init__(self):\n",
        "        print(\"Loading RoBERTa-base-MNLI…\")\n",
        "        self.nli = pipeline(\n",
        "            \"text-classification\",\n",
        "            model=\"textattack/roberta-base-MNLI\",\n",
        "            tokenizer=\"textattack/roberta-base-MNLI\",\n",
        "            device=0 if torch.cuda.is_available() else -1\n",
        "        )\n",
        "\n",
        "    def verify(self, premise: str, hypothesis: str) -> Dict:\n",
        "        input_text = f\"{premise} [SEP] {hypothesis}\"\n",
        "        result = self.nli(input_text)[0]\n",
        "        label = result[\"label\"].lower()   # FIXED\n",
        "        score = result[\"score\"]\n",
        "\n",
        "        return {\n",
        "            \"label\": label,\n",
        "            \"score\": score,\n",
        "            \"verified\": label == \"entailment\" and score > 0.80\n",
        "        }\n",
        "\n",
        "    def verify_response(self, response: str, context: List[Dict]) -> Dict:\n",
        "        combined = \" \".join([c[\"text\"] for c in context])\n",
        "        sentences = [s.strip() for s in response.replace(\"\\n\", \". \").split(\".\") if s.strip()]\n",
        "\n",
        "        if not sentences:\n",
        "            return {\"verified\": True, \"entailment_score\": 1.0, \"details\": []}\n",
        "\n",
        "        results = [self.verify(combined, s) for s in sentences]\n",
        "        entail_rate = sum(r[\"verified\"] for r in results) / len(results)\n",
        "\n",
        "        return {\n",
        "            \"verified\": entail_rate >= 0.70,\n",
        "            \"entailment_score\": entail_rate,\n",
        "            \"details\": results\n",
        "        }\n"
      ],
      "metadata": {
        "id": "7Sm9Gda3v3oW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ──────────────────────────────────────────────────────────────\n",
        "# CELL 4: Your Document Paths\n",
        "# ──────────────────────────────────────────────────────────────\n",
        "document_paths = [\n",
        "    \"/content/drive/MyDrive/Tb_Documents/Global Tuberculosis Report 2024 by WHO.pdf\",\n",
        "    \"/content/drive/MyDrive/Tb_Documents/Management_of_Tuberculosis_(4th_Edition) by MHO.pdf\"\n",
        "]\n",
        "\n",
        "# Verify files exist\n",
        "for p in document_paths:\n",
        "    assert os.path.exists(p), f\"File not found: {p}\"\n",
        "print(\"All files found!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "63mAihBgwX7-",
        "outputId": "d6462fac-d91e-4acb-a65b-f6a455396ffe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All files found!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ──────────────────────────────────────────────────────────────\n",
        "# CELL 5: Build Index\n",
        "# ──────────────────────────────────────────────────────────────\n",
        "processor = DocumentProcessor(chunk_size=400)\n",
        "chunks = processor.process_documents(document_paths)\n",
        "print(f\"Total chunks: {len(chunks)}\")\n",
        "\n",
        "retriever = DPRRetriever()\n",
        "retriever.build_index(chunks)\n",
        "retriever.save_index(\"index\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NUF-qaNe0jfx",
        "outputId": "12240ed0-b9ed-40c4-98c6-c82bebb0c827"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading: /content/drive/MyDrive/Tb_Documents/Global Tuberculosis Report 2024 by WHO.pdf\n",
            "Loading: /content/drive/MyDrive/Tb_Documents/Management_of_Tuberculosis_(4th_Edition) by MHO.pdf\n",
            "Total chunks: 1256\n",
            "Loading DPR context encoder: facebook/dpr-ctx_encoder-multiset-base\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'DPRQuestionEncoderTokenizer'. \n",
            "The class this function is called from is 'DPRContextEncoderTokenizerFast'.\n",
            "Some weights of the model checkpoint at facebook/dpr-ctx_encoder-multiset-base were not used when initializing DPRContextEncoder: ['ctx_encoder.bert_model.pooler.dense.bias', 'ctx_encoder.bert_model.pooler.dense.weight']\n",
            "- This IS expected if you are initializing DPRContextEncoder from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DPRContextEncoder from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Encoding 1256 chunks (max 512 tokens)…\n",
            "  → 1256/1256\n",
            "FAISS index built – 1256 vectors\n",
            "Index saved to index/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ──────────────────────────────────────────────────────────────\n",
        "# CELL 6: Download Index\n",
        "# ──────────────────────────────────────────────────────────────\n",
        "!zip -r index.zip index/\n",
        "from google.colab import files\n",
        "files.download(\"index.zip\")\n",
        "print(\"DOWNLOAD COMPLETE! Unzip in VS Code.\")"
      ],
      "metadata": {
        "id": "aQxiQY9Q72kE",
        "outputId": "a0177197-4803-41d8-ca93-6588357d6309",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 104
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  adding: index/ (stored 0%)\n",
            "  adding: index/faiss.index (deflated 7%)\n",
            "  adding: index/chunks.json (deflated 72%)\n",
            "  adding: index/embeddings.npy (deflated 7%)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_196716e8-785f-4708-9df0-57028334352f\", \"index.zip\", 7324052)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DOWNLOAD COMPLETE! Unzip in VS Code.\n"
          ]
        }
      ]
    }
  ]
}